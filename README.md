# Multimodal-Application-Demo

# ðŸ“¸ Image Captioning App using BLIP (Assignment 5.2)

This project is a simple multimodal image captioning application that uses the [BLIP (Bootstrapping Language-Image Pretraining)](https://huggingface.co/Salesforce/blip-image-captioning-base) model from Hugging Face. The app is built using Python and [Gradio](https://gradio.app/) to provide a user-friendly web interface where users can upload an image and receive a natural language caption.

---

## ðŸš€ Features

- Upload an image and receive an AI-generated caption.
- Uses a pretrained vision-language model: **BLIP (base)**.
- Fully interactive web UI with **Gradio**.
- CUDA support for GPU acceleration (if available).

---

## ðŸ§  Model Description

**BLIP** is a vision-language model designed for tasks like:
- Image captioning
- Visual question answering (VQA)
- Image-text retrieval

BLIP processes images with a visual encoder and generates descriptive captions using a language model decoder.

---

## ðŸ–¼ï¸ Demo Screenshot

> ðŸ“Œ Insert your Gradio app screenshot below:

![Demo Screenshot](screenshot.png) <!-- Replace with actual file path if hosted -->

---

## ðŸ’» Tech Stack

- Python
- [Transformers](https://github.com/huggingface/transformers)
- [Gradio](https://github.com/gradio-app/gradio)
- PyTorch
- safetensors

---

